{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pdftotext.PDF at 0x1106fa120>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pdftotext\n",
    "\n",
    "# Load your PDF\n",
    "with open(\"../src/data/tool/test.pdf\", \"rb\") as f:\n",
    "    pdf = pdftotext.PDF(f)\n",
    "\n",
    "# How many pages?\n",
    "print(len(pdf))\n",
    "\n",
    "# Iterate over all the pages\n",
    "for page in pdf:\n",
    "    print(page)\n",
    "\n",
    "# Read some individual pages\n",
    "print(pdf[0])\n",
    "print(pdf[1])\n",
    "\n",
    "# Read all the text into one string\n",
    "print(\"\\n\\n\".join(pdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                           An annotation scheme for citation function\\n                        Simone Teufel               Advaith Siddharthan                           Dan Tidhar\\n                            Natural Language and Information Processing Group\\n                                                  Computer Laboratory\\n                                        Cambridge University, CB3 0FD, UK\\n      {Simone.Teufel,Advaith.Siddharthan,Dan.Tidhar}@cl.cam.ac.uk\\n                         Abstract                                     Li and Abe 96                   Brown et al. 90a          Church and Gale 91\\n                                                                                    Resnik 95                         Rose et al. 90\\n      We study the interplay of the discourse struc-\\n      ture of a scientific argument with formal ci-                                                                                  Dagan et al. 94\\n      tations. One subproblem of this is to clas-                   Hindle 93                Nitta and Niwa 94      Dagan et al 93\\n      sify academic citations in scientific articles ac-\\n      cording to their rhetorical function, e.g., as a\\n      rival approach, as a part of the solution, or\\n      as a flawed approach that justifies the cur-\\n                                                                    Hindle 90\\n      rent research. Here, we introduce our anno-                                                    Pereira et al. 93\\n      tation scheme with 12 categories, and present                       His notion of similarity\\n                                                                          seems to agree with our              Following Pereira et al, we measure\\n      an agreement study.                                                 intuitions in many cases,            word similarity by the relative entropy\\n                                                                          but it is not clear how it           or Kulbach−Leibler (KL) distance, bet−\\n                                                                          can be used directly to\\n1 Scientific writing, discourse structure                                 construct word classes\\n                                                                                                               ween the corresponding conditional\\n                                                                                                               distributions.\\n                                                                          and corresponding\\n      and citations                                                       models of association.\\nIn recent years, there has been increasing interest in\\n                                                                                    Figure 1: A rhetorical citation map\\napplying natural language processing technologies to\\nscientific literature. The overwhelmingly large num-\\nber of papers published in fields like biology, genetics            sentences (i.e., the sentence expressing the contrast or\\nand chemistry each year means that researchers need                 continuation would be outside the CiteSeer snippet).\\ntools for information access (extraction, retrieval, sum-           We present here an approach which uses the classifica-\\nmarization, question answering etc). There is also in-              tion of citations to help provide relational information\\ncreased interest in automatic citation indexing, e.g.,              across papers.\\nthe highly successful search tools Google Scholar and                  Citations play a central role in the process of writing\\nCiteSeer (Giles et al., 1998).1 This general interest in            a paper. Swales (1990) argues that scientific writing\\nimproving access to scientific articles fits well with re-          follows a general rhetorical argumentation structure:\\nsearch on discourse structure, as knowledge about the               researchers must justify that their paper makes a con-\\noverall structure and goal of papers can guide better in-           tribution to the knowledge in their discipline. Several\\nformation access.                                                   argumentation steps are required to make this justifica-\\n   Shum (1998) argues that experienced researchers are              tion work, e.g., the statement of their specific goal in\\noften interested in relations between articles. They                the paper (Myers, 1992). Importantly, the authors also\\nneed to know if a certain article criticises another and            must relate their current work to previous research, and\\nwhat the criticism is, or if the current work is based              acknowledge previous knowledge claims; this is done\\non that prior work. This type of information is hard                with a formal citation, and with language connecting\\nto come by with current search technology. Neither                  the citation to the argument, e.g., statements of usage of\\nthe author’s abstract, nor raw citation counts help users           other people’s approaches (often near textual segments\\nin assessing the relation between articles. And even                in the paper where these approaches are described), and\\nthough CiteSeer shows a text snippet around the phys-               statements of contrast with them (particularly in the\\nical location for searchers to peruse, there is no guar-            discussion or related work sections). We argue that the\\nantee that the text snippet provides enough information             automatic recognition of citation function is interest-\\nfor the searcher to infer the relation. In fact, studies            ing for two reasons: a) it serves to build better citation\\nfrom our annotated corpus (Teufel, 1999), show that                 indexers and b) in the long run, it will help constrain\\n69% of the 600 sentences stating contrast with other                interpretations of the overall argumentative structure of\\nwork and 21% of the 246 sentences stating research                  a scientific paper.\\ncontinuation with other work do not contain the cor-                   Being able to interpret the rhetorical status of a ci-\\nresponding citation; the citation is found in preceding             tation at a glance would add considerable value to ci-\\n    1\\n      CiteSeer automatically citation-indexes all scientific ar-    tation indexes, as shown in Fig. 1. Here differences\\nticles reached by a web-crawler, making them available to           and similarities are shown between the example paper\\nsearchers via authors or keywords in the title.                     (Pereira et al., 1993) and the papers it cites, as well as\\n                                                                 80\\n                     Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 80–87,\\n                            Sydney, July 2006. c 2006 Association for Computational Linguistics\\n the papers that cite it. Contrastive links are shown in            1.   Cited source is mentioned in the introduction or\\n                                                                        discussion as part of the history and state of the\\ngrey – links to rival papers and papers the current pa-\\n                                                                        art of the research question under investigation.\\nper contrasts itself to. Continuative links are shown in\\nblack – links to papers that are taken as starting point of        2.   Cited source is the specific point of departure for\\n                                                                        the research question investigated.\\nthe current research, or as part of the methodology of\\nthe current paper. The most important textual sentence             3.   Cited source contains the concepts, definitions,\\n                                                                        interpretations used (and pertaining to the disci-\\nabout each citation could be extracted and displayed.                   pline of the citing article).\\nFor instance, we see which aspect of Hindle (1990) the\\n                                                                   4.   Cited source contains the data (pertaining to the\\nPereira et al. paper criticises, and in which way Pereira               discipline of the citing article) which are used\\net al.’s work was used by Dagan et al. (1994).                          sporadically in the article.\\n   We present an annotation scheme for citations, based\\n                                                                   5.   Cited source contains the data (pertaining to the\\non empirical work in content citation analysis, which                   discipline of the citing particle) which are used\\nfits into this general framework of scientific argument                 for comparative purposes, in tables and statistics.\\nstructure. It consists of 12 categories, which allow us            6.   Cited source contains data and material (from\\nto mark the relationships of the current paper with the                 other disciplines than citing article) which is\\ncited work. Each citation is labelled with exactly one                  used sporadically in the citing text, in tables or\\ncategory. The following top-level four-way distinction                  statistics.\\napplies:                                                           7.   Cited source contains the method used.\\n                                                                   8.   Cited source substantiated a statement or assump-\\n   • Weakness: Authors point out a weakness in cited                    tion, or points to further information.\\n      work\\n                                                                   9.   Cited source is positively evaluated.\\n   • Contrast: Authors make contrast/comparison with              10.   Cited source is negatively evaluated.\\n      cited work (4 categories)                                   11.   Results of citing article prove, verify, substantiate\\n                                                                        the data or interpretation of cited source.\\n   • Positive: Authors agree with/make use of/show\\n      compatibility or similarity with cited work (6 cat-         12.   Results of citing article disprove, put into ques-\\n                                                                        tion the data as interpretation of cited source.\\n      egories), and\\n                                                                  13.   Results of citing article furnish a new interpreta-\\n   • Neutral: Function of citation is either neutral, or                tion/explanation to the data of the cited source.\\n      weakly signalled, or different from the three func-\\n      tions stated above.\\n                                                               Figure 2: Spiegel-Rüsing’s (1977) Categories for Cita-\\n   We first turn to the point of how to classify citation      tion Motivations\\nfunction in a robust way. Later in this paper, we will\\nreport results for a human annotation experiment with\\nthree annotators.                                              particular early, basic paper, which gives the founda-\\n                                                               tion of their current subject (“paying homage to pio-\\n2 Annotation schemes for citations                             neers”). Many classification schemes for citation func-\\nIn the field of library sciences (more specifically, the       tions have been developed (Weinstock, 1971; Swales,\\nfield of Content Citation Analysis), the use of informa-       1990; Oppenheim and Renn, 1978; Frost, 1979; Chu-\\ntion from citations above and beyond simple citation           bin and Moitra, 1975), inter alia. Based on such an-\\ncounting has received considerable attention. Biblio-          notation schemes and hand-analyzed data, different in-\\nmetric measures assesses the quality of a researcher’s         fluences on citation behaviour can be determined, but\\noutput, in a purely quantitative manner, by counting           annotation in this field is usually done manually on\\nhow many papers cite a given paper (White, 2004;               small samples of text by the author, and not confirmed\\nLuukkonen, 1992) or by more sophisticated measures             by reliability studies. As one of the earliest such stud-\\nlike the h-index (Hirsch, 2005). But not all citations         ies, Moravcsik and Murugesan (1975) divide citations\\nare alike. Researchers in content citation analysis have       in running text into four dimensions: conceptual or\\nlong stated that the classification of motivations is a        operational use (i.e., use of theory vs. use of techni-\\ncentral element in understanding the relevance of the          cal method); evolutionary or juxtapositional (i.e., own\\npaper in the field. Bonzi (1982), for example, points out      work is based on the cited work vs. own work is an al-\\nthat negational citations, while pointing to the fact that     ternative to it); organic or perfunctory (i.e., work is cru-\\na given work has been noticed in a field, do not mean          cially needed for understanding of citing article or just\\nthat that work is received well, and Ziman (1968) states       a general acknowledgement); and finally confirmative\\nthat many citations are done out of “politeness” (to-          vs. negational (i.e., is the correctness of the findings\\nwards powerful rival approaches), “policy” (by name-           disputed?). They found, for example, that 40% of the\\ndropping and argument by authority) or “piety” (to-            citations were perfunctory, which casts further doubt\\nwards one’s friends, collaborators and superiors). Re-         on the citation-counting approach.\\nsearchers also often follow the custom of citing some             Other content citation analysis research which is rel-\\n                                                            81\\n evant to our work concentrates on relating textual spans       cation we have in mind. A third criterion is that they\\nto authors’ descriptions of other work. For example, in        should have some (theoretical) relation to the particu-\\nO’Connor’s (1982) experiment, citing statements (one           lar discourse structure we work with (Teufel, 1999).\\nor more sentences referring to other researchers’ work)           Our categories are as follows: One category (Weak)\\nwere identified manually. The main problem encoun-             is reserved for weakness of previous research, if it is ad-\\ntered in that work is the fact that many instances of cita-    dressed by the authors (cf. Spiegel-Rüsing’s categories\\ntion context are linguistically unmarked. Our data con-        10, 12, possibly 13). The next three categories describe\\nfirms this: articles often contain large segments, par-        comparisons or contrasts between own and other work\\nticularly in the central parts, which describe other peo-      (cf. Spiegel-Rüsing’s category 5). The difference be-\\nple’s research in a fairly neutral way. We would thus          tween them concerns whether the comparison is be-\\nexpect many citations to be neutral (i.e., not to carry        tween methods/goals (CoCoGM) or results (CoCoR0).\\nany function relating to the argumentation per se).            These two categories are for comparisons without ex-\\n   Many of the distinctions typically made in content          plicit value judgements. We use a different category\\ncitation analysis are immaterial to the task considered        (CoCo-) when the authors claim their approach is bet-\\nhere as they are too sociologically orientated, and can        ter than the cited work.\\nthus be difficult to operationalise without deep knowl-           Our interest in differences and similarities between\\nedge of the field and its participants (Swales, 1986). In      approaches stems from one possible application we\\nparticular, citations for general reference (background        have in mind (the rhetorical citation search tool). We\\nmaterial, homage to pioneers) are not part of our an-          do not only consider differences stated between the cur-\\nalytic interest here, and so are citations “in passing”,       rent work and other work, but we also mark citations if\\nwhich are only marginally related to the argumentation         they are explicitly compared and contrasted with other\\nof the overall paper (Ziman, 1968).                            work (not the current paper). This is expressed in cat-\\n   Spiegel-Rüsing’s (1977) scheme (Fig. 2) is an exam-        egory CoCoXY. It is a category not typically consid-\\nple of a scheme which is easier to operationalise than         ered in the literature, but it is related to the other con-\\nmost. In her scheme, more than one category can apply          trastive categories, and useful to us because we think\\nto a citation; for instance positive and negative evalu-       it can be exploited for search of differences and rival\\nation (category 9 and 10) can be cross-classified with         approaches.\\nother categories. Out of 2309 citations examined, 80%\\n                                                                  The next set of categories we propose concerns pos-\\nsubstantiated statements (category 8), 6% discussed\\n                                                               itive sentiment expressed towards a citation, or a state-\\nhistory or state of the art of the research area (cate-\\n                                                               ment that the other work is actively used in the cur-\\ngory 1) and 5% cited comparative data (category 5).\\n                                                               rent work (which is the ultimate praise). Like Spiegel-\\n                                                               Rüsing, we are interested in use of data and methods\\n  Category     Description\\n  Weak         Weakness of cited approach                      (her categories 4, 5, 6, 7), but we cluster different us-\\n  CoCoGM       Contrast/Comparison in Goals or Meth-           ages together and instead differentiate unchanged use\\n               ods (neutral)                                   (PUse) from use with adaptations (PModi). Work\\n  CoCoR0       Contrast/Comparison in Results (neutral)        which is stated as the explicit starting point or intellec-\\n  CoCo-        Unfavourable Contrast/Comparison (cur-          tual ancestry is marked with our category PBas (her\\n               rent work is better than cited work)\\n  CoCoXY       Contrast between 2 cited methods\\n                                                               category 2). If a claim in the literature is used to\\n  PBas         author uses cited work as starting point        strengthen the authors’ argument, this is expressed in\\n  PUse         author uses tools/algorithms/data               her category 8, and vice versa, category 11. We col-\\n  PModi        author        adapts       or      modifies     lapse these two in our category PSup. We use two\\n               tools/algorithms/data                           categories she does not have definitions for, namely\\n  PMot         this citation is positive about approach or\\n                                                               similarity of (aspect of) approach to other approach\\n               problem addressed (used to motivate work\\n               in current paper)                               (PSim), and motivation of approach used or problem\\n  PSim         author’s work and cited work are similar        addressed (PMot). We found evidence for prototypi-\\n  PSup         author’s work and cited work are compat-        cal use of these citation functions in our texts. How-\\n               ible/provide support for each other             ever, we found little evidence for her categories 12 or\\n  Neut         Neutral description of cited work, or not\\n                                                               13 (disproval or new interpretation of claims in cited\\n               enough textual evidence for above cate-\\n               gories or unlisted citation function            literature), and we decided against a “state-of-the-art”\\n                                                               category (her category 1), which would have been in\\n Figure 3: Our annotation scheme for citation function         conflict with our PMot definition in many cases.\\n                                                                  Our fourteenth category, Neut, bundles truly neutral\\n   Our scheme (given in Fig. 3) is an adaptation of the        descriptions of other researchers’ approaches with all\\nscheme in Fig. 2, which we arrived at after an analysis        those cases where the textual evidence for a citation\\nof a corpus of scientific articles in computational lin-       function was not enough to warrant annotation of that\\nguistics. We tried to redefine the categories such that        category, and all other functions for which our scheme\\nthey should be reasonably reliably annotatable; at the         did not provide a specific category. As stated above, we\\nsame time, they should be informative for the appli-           do in fact expect many of our citations to be neutral.\\n                                                            82\\n    Citation function is hard to annotate because it in          research as starting point, as intellectual ancestry, i.e.\\nprinciple requires interpretation of author intentions          PBas) and simply using it (PUse). In principle, one\\n(what could the author’s intention have been in choos-          would hope that annotation of all usage/positive cate-\\ning a certain citation?). Typical results of earlier cita-      gories (starting with P), if clustered together, should re-\\ntion function studies are that the sociological aspect of       sult in higher agreement (as they are similar, and as the\\nciting is not to be underestimated. One of our most fun-        resulting scheme has fewer distinctions). We would ex-\\ndamental ideas for annotation is to only mark explicitly        pect this to be the case in general, but as always, cases\\nsignalled citation functions. Our guidelines explicitly         exist where a conflict between a contrast (CoCo) and a\\nstate that a general linguistic phrase such as “better”         change to a method (PModi) occur:\\nor “used by us” must be present, in order to increase\\nobjectivity in finding citation function. Annotators are               In contrast to McCarthy, Kay and Kiraz,\\nencouraged to point to textual evidence they have for                  we combine the three components into a sin-\\nassigning a particular function (and are asked to type                 gle projection.               (0006044, S-182)\\nthe source of this evidence into the annotation tool for           The markable units in our scheme are a) all full cita-\\neach citation). Categories are defined in terms of cer-         tions (as recognized by our automatic citation proces-\\ntain objective types of statements (e.g., there are 7 cases     sor on our corpus), and b) all names of authors of cited\\nfor PMot). Annotators can use general text interpreta-          papers anywhere in running text outside of a formal\\ntion principles when assigning the categories, but are          citation context (i.e., without date). Our citation pro-\\nnot allowed to use in-depth knowledge of the field or           cessor recognizes these latter names after parsing the\\nof the authors.                                                 citation list an marks them up. This is unusual in com-\\n   There are other problematic aspects of the annota-           parison to other citation indexers, but we believe these\\ntion. Some concern the fact that authors do not al-             names function as important referents comparable in\\nways state their purpose clearly. For instance, several         importance to formal citations. In principle, one could\\nearlier studies found that negational citations are rare        go even further as there are many other linguistic ex-\\n(Moravcsik and Murugesan, 1975; Spiegel-Rüsing,                pressions by which the authors could refer to other peo-\\n1977); MacRoberts and MacRoberts (1984) argue that              ple’s work: pronouns, abbreviations such as “Mueller\\nthe reason for this is that they are potentially politically    and Sag (1990), henceforth M & S”, and names of ap-\\ndangerous, and that the authors go through lengths to           proaches or theories which are associated with partic-\\ndiffuse the impact of negative references, hiding a neg-        ular authors. If we could mark all of these up auto-\\native point behind insincere praise, or diffusing the           matically (which is not technically possible), annota-\\nthrust of criticism with perfunctory remarks. In our            tion would become less difficult to decide, but techni-\\ndata we found ample evidence of this effect, illustrated        cal difficulty prevent us from recognizing these other\\nby the following example:                                       cases automatically. As a result, in these contexts it is\\n       Hidden Markov Models (HMMs) (Huang                       impossible to annotate citation function directly on the\\n       et al. 1990) offer a powerful statistical ap-            referent, which sometimes causes problems. Because\\n       proach to this problem, though it is unclear             this means that annotators have to consider non-local\\n       how they could be used to recognise the units            context, one markable may have different competing\\n       of interest to phonologists. (9410022, S-24)2            contexts with different potential citation functions, and\\n                                                                problems about which context is “stronger” may oc-\\n   It is also sometimes extremely hard to distinguish           cur. We have rules that context is to be constrained to\\nusage of a method from statements of similarity be-             the paragraph boundary, but for some categories paper-\\ntween a method and the own method. This happens                 wide information is required (e.g., for PMot, we need\\nin cases where authors do not want to admit they are            to know that a praised approach is used by the authors,\\nusing somebody else’s method:                                   information which may not be local in the paragraph).\\n       The same test was used in Abney and Light                   Appendix A gives unambiguous example cases\\n       (1999).                      (0008020, S-151)            where the citation function can be decided on the ba-\\n       Unification of indices proceeds in the same              sis of the sentence alone, but Fig. 4 shows a more typ-\\n       manner as unification of all other typed                 ical example where more context is required to inter-\\n       feature structures (Carpenter 1992).                     pret the function. The evaluation of the citation Hin-\\n                                     (0008023, S-87)            dle (1990) is contrastive; the evaluative statement is\\n                                                                found 4 sentences after the sentence containing the ci-\\n   In this case, our annotators had to choose between           tation3 . It consists of a positive statement (agreement\\ncategories PSim and PUse.                                       with authors’ view), followed by a weakness, under-\\n   It can also be hard to distinguish between continu-          lined, which is the chosen category. This is marked on\\nation of somebody’s research (i.e., taking somebody’s           the nearest markable (Hindle, 3 sentences after the ci-\\n    2\\n                                                                tation).\\n      In all corpus examples, numbers in brackets correspond\\n                                                                    3\\nto the official Cmp lg archive number, “S-” numbers to sen-           In Fig. 4, markables are shown in boxes, evaluative state-\\ntence numbers according to our preprocessing.                   ments underlined, and referents in bold face.\\n                                                             83\\n   S-5 Hindle (1990)/Neut proposed dealing with the             overlap of citing and cited authors. The citation pro-\\n  sparseness problem by estimating the likelihood of un-       cessor developped in our group (Ritchie et al., 2006)\\n  seen events from that of “similar” events that have been     achieves high accuracy for this task (96% of citations\\n  seen.                                                        recognized, provided the reference list was error-free).\\n  S-6 For instance, one may estimate the likelihood of a\\n  particular direct object for a verb from the likelihoods     On average, our papers contain 26.8 citation instances\\n  of that direct object for similar verbs.                     in running text4 .\\n  S-7 This requires a reasonable definition of verb simi-\\n  larity and a similarity estimation method.                   4 Human Annotation: results\\n  S-8 In Hindle/Weak ’s proposal, words are similar\\n  if we have strong statistical evidence that they tend to     In order to machine learn citation function, we are\\n  participate in the same events.                              in the process of creating a corpus of scientific arti-\\n  S-9 His notion of similarity seems to agree with our in-     cles with human annotated citations, according to the\\n  tuitions in many cases, but it is not clear how it can be    scheme discussed before. Here we report preliminary\\n  used directly to construct word classes and correspond-\\n                                                               results with that scheme, with three annotators who are\\n  ing models of association.                    (9408011)\\n                                                               developers of the scheme.\\n  Figure 4: Annotation example: influence of context              In our experiment, the annotators independently an-\\n                                                               notated 26 conference articles with this scheme, on the\\n                                                               basis of guidelines which were frozen once annotation\\n    A naive view on this annotation scheme could con-          started5 . The data used for the experiment contained a\\nsider the first two sets of categories in our scheme as        total of 120,000 running words and 548 citations.\\n“negative” and the third set of categories “positive”.            The relative frequency of each category observed in\\nThere is indeed a sentiment aspect to the interpretation       the annotation is listed in Fig. 5. As expected, the dis-\\nof citations, due to the fact that authors need to make        tribution is very skewed, with more than 60% of the\\na point in their paper and thus have a stance towards          citations of category Neut.6 What is interesting is the\\ntheir citations. But this is not the whole story: many         relatively high frequency of usage categories (PUse,\\nof our “positive” categories are more concerned with           PModi, PBas) with a total of 18.9%. There is\\ndifferent ways in which the cited work is useful to the        a relatively low frequency of clearly negative cita-\\ncurrent work (which aspect of it is used, e.g., just a         tions (Weak, CoCoR-, total of 4.1%), whereas the\\ndefinition or the entire solution?), and many of the con-      neutral–contrastive categories (CoCoGM, CoCoR0,\\ntrastive statements have no negative connotation at all        CoCoXY) are slightly more frequent at 7.6%. This\\nand simply state a (value-free) difference between ap-         is in concordance with earlier annotation experiments\\nproaches. However, if one looks at the distribution of         (Moravcsik and Murugesan, 1975; Spiegel-Rüsing,\\npositive and negative adjectives around citations, one         1977).\\nnotices a (non-trivial) connection between our task and           We reached an inter-annotator agreement of K=.72\\nsentiment classification.                                      (n=12;N=548;k=3)7. This is comparable to aggreement\\n    There are written guidelines of 25 pages, which in-        on other discourse annotation tasks such as dialogue\\nstruct the annotators to only assign one category per          act parsing and Argumentative Zoning (Teufel et al.,\\ncitation, and to skim-read the paper before annotation.        1999). We consider the agreement quite good, consid-\\nThe guidelines provide a decision tree and give deci-          ering the number of categories and the difficulties (e.g.,\\nsion aids in systematically ambiguous cases, but sub-          non-local dependencies) of the task.\\njective judgement of the annotators is nevertheless nec-          The annotators are obviously still disagreeing on\\nessary to assign a single tag in an unseen context. We         some categories. We were wondering to what de-\\nimplemented an annotation tool based on XML/XSLT               gree the fine granularity of the scheme is a prob-\\ntechnology, which allows us to use any web browser to          lem. When we collapsed the obvious similar cat-\\ninteractively assign one of the 12 tags (presented as a        egories (all P categories into one category, and\\npull-down list) to each citation.                              all CoCo categories into another) to give four top\\n                                                               level categories (Weak, Positive, Contrast,\\n                                                               Neutral), this only raised kappa to 0.76. This\\n3 Data\\n                                                                   4\\n                                                                     As opposed to reference list items, which are fewer.\\nThe data we used came from the CmpLg (Computation                  5\\n                                                                     The development of the scheme was done with 40+ dif-\\nand Language archive; 320 conference articles in com-          ferent articles.\\nputational linguistics). The articles are in XML format.           6\\n                                                                     Spiegel-Rüsing found that out of 2309 citations she ex-\\nHeadlines, titles, authors and reference list items are        amined, 80% substantiated statements.\\n                                                                   7\\nautomatically marked up with the corresponding tags.                 Following Carletta (1996), we measure agreement in\\nReference lists are parsed, and cited authors’ names           Kappa, which follows the formula K = P (A)−P        (E)\\n                                                                                                             1−P (E)\\n                                                                                                                       where\\nare identified. Our citation parser then applies regu-         P(A) is observed, and P(E) expected agreement. Kappa\\n                                                               ranges between -1 and 1. K=0 means agreement is only as\\nlar patterns and finds citations and other occurrences of      expected by chance. Generally, Kappas of 0.8 are considered\\nthe names of cited authors (without a date) in running         stable, and Kappas of .69 as marginally stable, according to\\ntext and marks them up. Self-citations are detected by         the strictest scheme applied in the field.\\n                                                            84\\n    Neut     PUse     CoCoGM       PSim      Weak     CoCoXY       PMot     PModi     PBas    PSup      CoCo-      CoCoR0\\n  62.7%    15.8%       3.9%       3.8%      3.1%      2.9%       2.2%      1.6%     1.5%     1.1%      1.0%        0.8%\\n                                         Figure 5: Distribution of the categories\\n              Weak    CoCo-    CoCoGM       CoCoR0    CoCoXY      PUse    PBas   PModi    PMot    PSim     PSup    Neut\\n  Weak          5                                                                                                    3\\n  CoCo-                  1         3\\n  CoCoGM                           23                                                                3\\n  CoCoR0                                        4\\n  CoCoXY                                                  1\\n  PUse                                                              86      6                        2       1      12\\n  PBas                                                                      3                                        2\\n  PModi                                                                             3\\n  PMot                                                                                      13                       4\\n  PSim                                                              3                               20               5\\n  PSup                   1                                          2                                        1\\n  Neut          6                  10           6         4         17      1               6        4              287\\n                                 Figure 6: Confusion matrix between two annotators\\npoints to the fact that most of our annotators disagreed                          Weakness      Positive    Neutral\\nabout whether to assign a more informative category                  Weakness          9           1           12\\nor Neut, the neutral fall-back category. Unfortunately,              Positive                     140           13\\nKappa is only partially sensitive to such specialised dis-           Neutral           4           30          339\\nagreements. While it will reward agreement with in-\\nfrequent categories more than agreement with frequent          Figure 7: Confusion matrix between two annotators;\\ncategories, it nevertheless does not allow us to weight        categories collapsed to reflect sentiment\\ndisagreements we care less about (Neut vs more in-\\nformative category) less than disagreements we do care\\na lot about (informative categories which are mutually         have only one case of confusion between positive and\\nexclusive, such as Weak and PSim).                             negative references to cited work. The vast majority of\\n   Fig. 6 shows a confusion matrix between the two an-         disagreements reflects genuine ambiguity as to whether\\nnotators who agreed most with each other. This again           the authors were trying to stay neutral or express a sen-\\npoints to the fact that a large proportion of the confu-       timent.\\nsion involves an informative category and Neut. The\\nissue with Neut and Weak is a point at hand: au-                            Distinction               Kappa\\nthors seem to often (deliberately or not) mask their in-                    PMot v. all others           .790\\ntended citation function with seemingly neutral state-                      CoCoGM v. all others         .765\\nments. Many statements of weakness of other ap-                             PUse v. all others           .761\\nproaches were stated in such caged terms that our anno-                     CoCoR0 v. all others         .746\\ntators disagreed about whether the signals given were                       Neut v. all others           .742\\n“explicit” enough.                                                          PSim v. all others           .649\\n   While our focus is not sentiment analysis, it is pos-                    PModi v. all others          .553\\nsible to conflate our 12 categories into three: positive,                   CoCoXY v. all others         .553\\nweakness and neutral by the following mapping:                              Weak v. all others           .522\\n                                                                            CoCo- v. all others          .462\\n                                                                            PBas v. all others           .414\\n                          Old Categories    New Category\\n                                                                            PSup v. all others           .268\\n                           Weak, CoCo-      Negative\\n  PMot, PUse, PBas, PModi, PSim, PSup       Positive\\n     CoCoGM, CoCoR0, CoCoXY, Neut           Neutral                     Figure 8: Distinctiveness of categories\\n   Thus negative contrasts and weaknesses are grouped             In an attempt to determine how well each cate-\\ninto Negative, while neutral contrasts are grouped             gory was defined, we created artificial splits of the\\ninto Neutral. All the positive classes are conflated           data into binary distinctions: each category versus a\\ninto Positive. This resulted in kappa=0.75 for three           super-category consisting of all the other collapsed cat-\\nannotators.                                                    egories. The kappas measured on these datasets are\\n   Fig. 7 shows the confusion matrix between two an-           given in Fig. 8. The higher they are, the better the anno-\\nnotators for this sentiment classification. Fig. 7 is par-     tators could distinguish the given category from all the\\nticularly instructive, because it shows that annotators        other categories. We can see that out of the informa-\\n                                                           85\\n tive categories, four are defined at least as well as the           tasks: The kappa statistic. Computational Linguistics,\\noverall distinction (i.e. above the line in Fig. 8: PMot,           22(2):249–254.\\nPUse, CoCoGM and CoCoR0. This is encouraging,                    Daryl E. Chubin and S. D. Moitra. 1975. Content analysis\\nas the application of citation maps is almost entirely              of references: Adjunct or alternative to citation counting?\\n                                                                    Social Studies of Science, 5(4):423–441.\\ncentered around usage and contrast. However, the se-\\n                                                                 Carolyn O. Frost. 1979. The use of citations in literary re-\\nmantics of some categories are less well-understood by              search: A preliminary classification of citation functions.\\nour annotators: in particular PSup (where the difficulty            Library Quarterly, 49:405.\\nlies in what an annotator understands as “mutual sup-            C. Lee Giles, Kurt D. Bollacker, and Steve Lawrence. 1998.\\nport” of two theories), and (unfortunately) PBas. The               Citeseer: An automatic citation indexing system. In Pro-\\nproblem with PBas is that its distinction from PUse is              ceedings of the Third ACM Conference on Digital Li-\\n                                                                    braries, pages 89–98.\\nbased on subjective judgement of whether the authors\\n                                                                 Jorge E. Hirsch. 2005. An index to quantify an individ-\\nuse a part of somebody’s previous work, or base them-               ual’s scientific research output. Proceedings of the Na-\\nselves entirely on this previous work (i.e., see them-              tional Academy of Sciences of the United Stated of Amer-\\nselves as following in the same intellectual framework).            ica (PNAS), 102(46).\\nAnother problem concerns the low distinctivity for the           Terttu Luukkonen. 1992. Is scientists’ publishing behaviour\\nclearly negative categories CoCo- and Weak. This is                 reward-seeking? Scientometrics, 24:297–319.\\nin line with MacRoberts and MacRoberts’ hypothesis               Michael H. MacRoberts and Barbara R. MacRoberts. 1984.\\nthat criticism is often hedged and not clearly lexically            The negational reference: Or the art of dissembling. So-\\n                                                                    cial Studies of Science, 14:91–94.\\nsignalled, which makes it more difficult to reliably an-\\n                                                                 Michael J. Moravcsik and Poovanalingan Murugesan. 1975.\\nnotate such citations.                                              Some results on the function and quality of citations. So-\\n                                                                    cial Studies of Science, 5:88–91.\\n5 Conclusion                                                     Greg Myers. 1992. In this paper we report...—speech acts\\n                                                                    and scientific facts. Journal of Pragmatics, 17(4):295–\\nWe have described a new task: human annotation of                   313.\\ncitation function, a phenomenon which we believe to              John O’Connor. 1982. Citing statements: Computer recogni-\\nbe closely related to the overall discourse structure of            tion and use to improve retrieval. Information Processing\\n                                                                    and Management, 18(3):125–131.\\nscientific articles. Our annotation scheme concentrates\\n                                                                 Charles Oppenheim and Susan P. Renn. 1978. Highly cited\\non contrast, weaknesses of other work, similarities be-             old papers and the reasons why they continue to be cited.\\ntween work and usage of other work. One of its prin-                JASIS, 29:226–230.\\nciples is the fact that relations are only to be marked if       Anna Ritchie, Simone Teufel, and Steven Robertson. 2006.\\nthey are explicitly signalled. Here, we report positive             Creating a test collection for citation-based IR experi-\\nresults in terms of interannotator agreement.                       ments. In Proceedings of HLT-06.\\n   Future work on the annotation scheme will concen-             Simon Buckingham Shum. 1998. Evolving the web for sci-\\n                                                                    entific knowledge: First steps towards an “HCI knowledge\\ntrate on improving guidelines for currently suboptimal\\n                                                                    web”. Interfaces, British HCI Group Magazine, 39:16–21.\\ncategories, and on measuring intra-annotator agree-              Ina Spiegel-Rüsing. 1977. Bibliometric and content analy-\\nment and inter-annotator agreement with naive annota-               sis. Social Studies of Science, 7:97–113.\\ntors. We are also currently investigating how well our           John Swales. 1986. Citation analysis and discourse analysis.\\nscheme will work on text from a different discipline,               Applied Linguistics, 7(1):39–56.\\nnamely chemistry. Work on applying machine learning              John Swales, 1990. Genre Analysis: English in Academic\\ntechniques for automatic citation classification is cur-            and Research Settings. Chapter 7: Research articles in En-\\nrently underway (Teufel et al., 2006); the agreement                glish, pages 110–176. Cambridge University Press, Cam-\\n                                                                    bridge, UK.\\nof one annotator and the system is currently K=.57,\\n                                                                 Simone Teufel, Jean Carletta, and Marc Moens. 1999. An\\nleaving plenty of room for improvement in comparison                annotation scheme for discourse-level argumentation in re-\\nwith the human annotation results presented here.                   search articles. In Proceedings of the Ninth Meeting of the\\n                                                                    European Chapter of the Association for Computational\\n6 Acknowledgements                                                  Linguistics (EACL-99), pages 110–117.\\n                                                                 Simone Teufel, Advaith Siddharthan, and Dan Tidhar. 2006.\\nThis work was funded by the EPSRC projects                          Automatic classification of citation function. In Proceed-\\nCITRAZ (GR/S27832/01, “Rhetorical Citation Maps                     ings of EMNLP-06.\\n                                                                 Simone Teufel. 1999. Argumentative Zoning: Information\\nand Domain-independent Argumentative Zoning”) and\\n                                                                    Extraction from Scientific Text. Ph.D. thesis, School of\\nSCIBORG (EP/C010035/1, “Extracting the Science                      Cognitive Science, University of Edinburgh, UK.\\nfrom Scientific Publications”).                                  Melvin Weinstock. 1971. Citation indexes. In Encyclopedia\\n                                                                    of Library and Information Science, volume 5, pages 16–\\n                                                                    40. Dekker, New York, NY.\\nReferences                                                       Howard D. White. 2004. Citation analysis and discourse\\n                                                                    analysis revisited. Applied Linguistics, 25(1):89–116.\\nSusan Bonzi. 1982. Characteristics of a literature as predic-    John M. Ziman. 1968. Public Knowledge: An Essay Con-\\n   tors of relatedness between cited and citing works. JASIS,       cerning the Social Dimensions of Science. Cambridge\\n   33(4):208–216.                                                   University Press, Cambridge, UK.\\nJean Carletta. 1996. Assessing agreement on classification\\n                                                              86\\n A  Annotation examples\\n Weak   However, Koskenniemi himself understood that his initial implementation had signif-\\n        icant limitations in handling non-concatenative morphotactic processes.\\n                                                                                 (0006044, S-4)\\n CoCoGM The goals of the two papers are slightly different: Moore ’s approach is designed to\\n        reduce the total grammar size (i.e., the sum of the lengths of the productions), while\\n        our approach minimizes the number of productions.\\n                                                                                (0008021, S-22)\\n CoCoR0 This is similar to results in the literature (Ramshaw and Marcus 1995).\\n                                                                               (0008022, S-147)\\n CoCo-  For the Penn Treebank, Ratnaparkhi (1996) reports an accuracy of 96.6% using the\\n        Maximum Entropy approach, our much simpler and therefore faster HMM approach\\n        delivers 96.7%.                                                        (0003055, S-156)\\n CoCoXY Unlike previous approaches (Ellison 1994, Walther 1996), Karttunen ’s approach\\n        is encoded entirely in the finite state calculus, with no extra-logical procedures for\\n        counting constraint violations.                                          (0006038, S-5)\\n PBas   Our starting point is the work described in Ferro et al. (1999) , which used a fairly\\n        small training set.                                                     (0008004, S-11)\\n PUse   In our application, we tried out the Learning Vector Quantization (LVQ) (Kohonen et\\n        al. 1996).                                                             (0003060, S-105)\\n PModi  In our experiments, we have used a conjugate-gradient optimization program adapted\\n        from the one presented in Press et al.                                  (0008028, S-72)\\n PMot   It has also been shown that the combined accuracy of an ensemble of multiple clas-\\n        sifiers is often significantly greater than that of any of the individual classifiers that\\n        make up the ensemble (e.g., Dietterich (1997)).                          (0005006, S-9)\\n PSim   Our system is closely related to those proposed in Resnik (1997) and Abney and\\n        Light (1999).                                                           (0008020, S-24)\\n PSup   In all experiments the SVM Light system outperformed other learning algorithms,\\n        which confirms Yang and Liu ’s (1999) results for SVMs fed with Reuters data.\\n                                                                               (0003060, S-141)\\n Neut   The cosine metric and Jaccard’s coefficient are commonly used in information re-\\n        trieval as measures of association (Salton and McGill 1983).            (0001012, S-29)\\n                                                      87\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 80\\xe2\\x80\\x9387,\\n\\nSydney, July 2006. c(cid:13)2006 Association for Computational Linguistics\\n\\n80\\n\\nAn annotation scheme for citation function\\n\\nSimone Teufel\\n\\nAdvaith Siddharthan\\n\\nDan Tidhar\\n\\nNatural Language and Information Processing Group\\n\\nComputer Laboratory\\n\\nCambridge University, CB3 0FD, UK\\n\\n{Simone.Teufel,Advaith.Siddharthan,Dan.Tidhar}@cl.cam.ac.uk\\n\\nAbstract\\n\\nWe study the interplay of the discourse struc-\\nture of a scienti(cid:2)c argument with formal ci-\\ntations. One subproblem of this is to clas-\\nsify academic citations in scienti(cid:2)c articles ac-\\ncording to their rhetorical function, e.g., as a\\nrival approach, as a part of the solution, or\\nas a (cid:3)awed approach that justi(cid:2)es the cur-\\nrent research. Here, we introduce our anno-\\ntation scheme with 12 categories, and present\\nan agreement study.\\n\\n1 Scienti(cid:2)c writing, discourse structure\\n\\nand citations\\n\\nIn recent years, there has been increasing interest in\\napplying natural language processing technologies to\\nscienti(cid:2)c literature. The overwhelmingly large num-\\nber of papers published in (cid:2)elds like biology, genetics\\nand chemistry each year means that researchers need\\ntools for information access (extraction, retrieval, sum-\\nmarization, question answering etc). There is also in-\\ncreased interest in automatic citation indexing, e.g.,\\nthe highly successful search tools Google Scholar and\\nCiteSeer (Giles et al., 1998).1 This general interest in\\nimproving access to scienti(cid:2)c articles (cid:2)ts well with re-\\nsearch on discourse structure, as knowledge about the\\noverall structure and goal of papers can guide better in-\\nformation access.\\n\\nShum (1998) argues that experienced researchers are\\noften interested in relations between articles. They\\nneed to know if a certain article criticises another and\\nwhat the criticism is, or if the current work is based\\non that prior work. This type of information is hard\\nto come by with current search technology. Neither\\nthe author\\xe2\\x80\\x99s abstract, nor raw citation counts help users\\nin assessing the relation between articles. And even\\nthough CiteSeer shows a text snippet around the phys-\\nical location for searchers to peruse, there is no guar-\\nantee that the text snippet provides enough information\\nfor the searcher to infer the relation. In fact, studies\\nfrom our annotated corpus (Teufel, 1999), show that\\n69% of the 600 sentences stating contrast with other\\nwork and 21% of the 246 sentences stating research\\ncontinuation with other work do not contain the cor-\\nresponding citation; the citation is found in preceding\\n1CiteSeer automatically citation-indexes all scienti(cid:2)c ar-\\nticles reached by a web-crawler, making them available to\\nsearchers via authors or keywords in the title.\\n\\nLi and Abe 96\\n\\nBrown et al. 90a\\n\\nChurch and Gale 91\\n\\nResnik 95\\n\\nRose et al. 90\\n\\nHindle 93\\n\\nNitta and Niwa 94\\n\\nDagan et al 93\\n\\nDagan et al. 94\\n\\nHindle 90\\n\\nPereira et al. 93\\n\\nHis notion of similarity\\nseems to agree with our\\nintuitions in many cases,\\nbut it is not clear how it\\ncan  be used directly to\\nconstruct word classes\\nand corresponding \\nmodels of association.\\n\\nFollowing Pereira et al, we measure\\nword similarity by the relative entropy\\nor Kulbach-Leibler (KL) distance, bet-\\nween the corresponding conditional\\ndistributions.\\n\\nFigure 1: A rhetorical citation map\\n\\nsentences (i.e., the sentence expressing the contrast or\\ncontinuation would be outside the CiteSeer snippet).\\nWe present here an approach which uses the classi(cid:2)ca-\\ntion of citations to help provide relational information\\nacross papers.\\n\\nCitations play a central role in the process of writing\\na paper. Swales (1990) argues that scienti(cid:2)c writing\\nfollows a general rhetorical argumentation structure:\\nresearchers must justify that their paper makes a con-\\ntribution to the knowledge in their discipline. Several\\nargumentation steps are required to make this justi(cid:2)ca-\\ntion work, e.g., the statement of their speci(cid:2)c goal in\\nthe paper (Myers, 1992). Importantly, the authors also\\nmust relate their current work to previous research, and\\nacknowledge previous knowledge claims; this is done\\nwith a formal citation, and with language connecting\\nthe citation to the argument, e.g., statements of usage of\\nother people\\xe2\\x80\\x99s approaches (often near textual segments\\nin the paper where these approaches are described), and\\nstatements of contrast with them (particularly in the\\ndiscussion or related work sections). We argue that the\\nautomatic recognition of citation function is interest-\\ning for two reasons: a) it serves to build better citation\\nindexers and b) in the long run, it will help constrain\\ninterpretations of the overall argumentative structure of\\na scienti(cid:2)c paper.\\n\\nBeing able to interpret the rhetorical status of a ci-\\ntation at a glance would add considerable value to ci-\\ntation indexes, as shown in Fig. 1. Here differences\\nand similarities are shown between the example paper\\n(Pereira et al., 1993) and the papers it cites, as well as\\n\\n\\x0c81\\n\\nthe papers that cite it. Contrastive links are shown in\\ngrey (cid:150) links to rival papers and papers the current pa-\\nper contrasts itself to. Continuative links are shown in\\nblack (cid:150) links to papers that are taken as starting point of\\nthe current research, or as part of the methodology of\\nthe current paper. The most important textual sentence\\nabout each citation could be extracted and displayed.\\nFor instance, we see which aspect of Hindle (1990) the\\nPereira et al. paper criticises, and in which way Pereira\\net al.\\xe2\\x80\\x99s work was used by Dagan et al. (1994).\\n\\nWe present an annotation scheme for citations, based\\non empirical work in content citation analysis, which\\n(cid:2)ts into this general framework of scienti(cid:2)c argument\\nstructure. It consists of 12 categories, which allow us\\nto mark the relationships of the current paper with the\\ncited work. Each citation is labelled with exactly one\\ncategory. The following top-level four-way distinction\\napplies:\\n\\n(cid:15) Weakness: Authors point out a weakness in cited\\n\\nwork\\n\\n(cid:15) Contrast: Authors make contrast/comparison with\\n\\ncited work (4 categories)\\n\\n(cid:15) Positive: Authors agree with/make use of/show\\ncompatibility or similarity with cited work (6 cat-\\negories), and\\n\\n(cid:15) Neutral: Function of citation is either neutral, or\\nweakly signalled, or different from the three func-\\ntions stated above.\\n\\nWe (cid:2)rst turn to the point of how to classify citation\\nfunction in a robust way. Later in this paper, we will\\nreport results for a human annotation experiment with\\nthree annotators.\\n2 Annotation schemes for citations\\nIn the (cid:2)eld of library sciences (more speci(cid:2)cally, the\\n(cid:2)eld of Content Citation Analysis), the use of informa-\\ntion from citations above and beyond simple citation\\ncounting has received considerable attention. Biblio-\\nmetric measures assesses the quality of a researcher\\xe2\\x80\\x99s\\noutput, in a purely quantitative manner, by counting\\nhow many papers cite a given paper (White, 2004;\\nLuukkonen, 1992) or by more sophisticated measures\\nlike the h-index (Hirsch, 2005). But not all citations\\nare alike. Researchers in content citation analysis have\\nlong stated that the classi(cid:2)cation of motivations is a\\ncentral element in understanding the relevance of the\\npaper in the (cid:2)eld. Bonzi (1982), for example, points out\\nthat negational citations, while pointing to the fact that\\na given work has been noticed in a (cid:2)eld, do not mean\\nthat that work is received well, and Ziman (1968) states\\nthat many citations are done out of (cid:147)politeness(cid:148) (to-\\nwards powerful rival approaches), (cid:147)policy(cid:148) (by name-\\ndropping and argument by authority) or (cid:147)piety(cid:148) (to-\\nwards one\\xe2\\x80\\x99s friends, collaborators and superiors). Re-\\nsearchers also often follow the custom of citing some\\n\\n1. Cited source is mentioned in the introduction or\\ndiscussion as part of the history and state of the\\nart of the research question under investigation.\\n2. Cited source is the speci(cid:2)c point of departure for\\n\\nthe research question investigated.\\n\\n3. Cited source contains the concepts, de(cid:2)nitions,\\ninterpretations used (and pertaining to the disci-\\npline of the citing article).\\n\\n4. Cited source contains the data (pertaining to the\\ndiscipline of the citing article) which are used\\nsporadically in the article.\\n\\n5. Cited source contains the data (pertaining to the\\ndiscipline of the citing particle) which are used\\nfor comparative purposes, in tables and statistics.\\n6. Cited source contains data and material (from\\nother disciplines than citing article) which is\\nused sporadically in the citing text, in tables or\\nstatistics.\\n\\n7. Cited source contains the method used.\\n8. Cited source substantiated a statement or assump-\\n\\ntion, or points to further information.\\n9. Cited source is positively evaluated.\\n10.\\nCited source is negatively evaluated.\\nResults of citing article prove, verify, substantiate\\n11.\\nthe data or interpretation of cited source.\\nResults of citing article disprove, put into ques-\\ntion the data as interpretation of cited source.\\nResults of citing article furnish a new interpreta-\\ntion/explanation to the data of the cited source.\\n\\n12.\\n\\n13.\\n\\nFigure 2: Spiegel-R\\xc2\\xa4using\\xe2\\x80\\x99s (1977) Categories for Cita-\\ntion Motivations\\n\\nparticular early, basic paper, which gives the founda-\\ntion of their current subject ((cid:147)paying homage to pio-\\nneers(cid:148)). Many classi(cid:2)cation schemes for citation func-\\ntions have been developed (Weinstock, 1971; Swales,\\n1990; Oppenheim and Renn, 1978; Frost, 1979; Chu-\\nbin and Moitra, 1975), inter alia. Based on such an-\\nnotation schemes and hand-analyzed data, different in-\\n(cid:3)uences on citation behaviour can be determined, but\\nannotation in this (cid:2)eld is usually done manually on\\nsmall samples of text by the author, and not con(cid:2)rmed\\nby reliability studies. As one of the earliest such stud-\\nies, Moravcsik and Murugesan (1975) divide citations\\nin running text into four dimensions: conceptual or\\noperational use (i.e., use of theory vs. use of techni-\\ncal method); evolutionary or juxtapositional (i.e., own\\nwork is based on the cited work vs. own work is an al-\\nternative to it); organic or perfunctory (i.e., work is cru-\\ncially needed for understanding of citing article or just\\na general acknowledgement); and (cid:2)nally con(cid:2)rmative\\nvs. negational (i.e., is the correctness of the (cid:2)ndings\\ndisputed?). They found, for example, that 40% of the\\ncitations were perfunctory, which casts further doubt\\non the citation-counting approach.\\n\\nOther content citation analysis research which is rel-\\n\\n\\x0c82\\n\\nevant to our work concentrates on relating textual spans\\nto authors\\xe2\\x80\\x99 descriptions of other work. For example, in\\nO\\xe2\\x80\\x99Connor\\xe2\\x80\\x99s (1982) experiment, citing statements (one\\nor more sentences referring to other researchers\\xe2\\x80\\x99 work)\\nwere identi(cid:2)ed manually. The main problem encoun-\\ntered in that work is the fact that many instances of cita-\\ntion context are linguistically unmarked. Our data con-\\n(cid:2)rms this: articles often contain large segments, par-\\nticularly in the central parts, which describe other peo-\\nple\\xe2\\x80\\x99s research in a fairly neutral way. We would thus\\nexpect many citations to be neutral (i.e., not to carry\\nany function relating to the argumentation per se).\\n\\nMany of the distinctions typically made in content\\ncitation analysis are immaterial to the task considered\\nhere as they are too sociologically orientated, and can\\nthus be dif(cid:2)cult to operationalise without deep knowl-\\nedge of the (cid:2)eld and its participants (Swales, 1986). In\\nparticular, citations for general reference (background\\nmaterial, homage to pioneers) are not part of our an-\\nalytic interest here, and so are citations (cid:147)in passing(cid:148),\\nwhich are only marginally related to the argumentation\\nof the overall paper (Ziman, 1968).\\n\\nSpiegel-R\\xc2\\xa4using\\xe2\\x80\\x99s (1977) scheme (Fig. 2) is an exam-\\nple of a scheme which is easier to operationalise than\\nmost. In her scheme, more than one category can apply\\nto a citation; for instance positive and negative evalu-\\nation (category 9 and 10) can be cross-classi(cid:2)ed with\\nother categories. Out of 2309 citations examined, 80%\\nsubstantiated statements (category 8), 6% discussed\\nhistory or state of the art of the research area (cate-\\ngory 1) and 5% cited comparative data (category 5).\\n\\nCategory Description\\nWeak\\nCoCoGM\\n\\nCoCoR0\\nCoCo-\\n\\nCoCoXY\\nPBas\\nPUse\\nPModi\\n\\nPMot\\n\\nPSim\\nPSup\\n\\nNeut\\n\\nWeakness of cited approach\\nContrast/Comparison in Goals or Meth-\\nods (neutral)\\nContrast/Comparison in Results (neutral)\\nUnfavourable Contrast/Comparison (cur-\\nrent work is better than cited work)\\nContrast between 2 cited methods\\nauthor uses cited work as starting point\\nauthor uses tools/algorithms/data\\nauthor\\ntools/algorithms/data\\nthis citation is positive about approach or\\nproblem addressed (used to motivate work\\nin current paper)\\nauthor\\xe2\\x80\\x99s work and cited work are similar\\nauthor\\xe2\\x80\\x99s work and cited work are compat-\\nible/provide support for each other\\nNeutral description of cited work, or not\\nenough textual evidence for above cate-\\ngories or unlisted citation function\\n\\nmodi(cid:2)es\\n\\nadapts\\n\\nor\\n\\nFigure 3: Our annotation scheme for citation function\\n\\nOur scheme (given in Fig. 3) is an adaptation of the\\nscheme in Fig. 2, which we arrived at after an analysis\\nof a corpus of scienti(cid:2)c articles in computational lin-\\nguistics. We tried to rede(cid:2)ne the categories such that\\nthey should be reasonably reliably annotatable; at the\\nsame time, they should be informative for the appli-\\n\\ncation we have in mind. A third criterion is that they\\nshould have some (theoretical) relation to the particu-\\nlar discourse structure we work with (Teufel, 1999).\\n\\nOur categories are as follows: One category (Weak)\\nis reserved for weakness of previous research, if it is ad-\\ndressed by the authors (cf. Spiegel-R\\xc2\\xa4using\\xe2\\x80\\x99s categories\\n10, 12, possibly 13). The next three categories describe\\ncomparisons or contrasts between own and other work\\n(cf. Spiegel-R\\xc2\\xa4using\\xe2\\x80\\x99s category 5). The difference be-\\ntween them concerns whether the comparison is be-\\ntween methods/goals (CoCoGM) or results (CoCoR0).\\nThese two categories are for comparisons without ex-\\nplicit value judgements. We use a different category\\n(CoCo-) when the authors claim their approach is bet-\\nter than the cited work.\\n\\nOur interest in differences and similarities between\\napproaches stems from one possible application we\\nhave in mind (the rhetorical citation search tool). We\\ndo not only consider differences stated between the cur-\\nrent work and other work, but we also mark citations if\\nthey are explicitly compared and contrasted with other\\nwork (not the current paper). This is expressed in cat-\\negory CoCoXY. It is a category not typically consid-\\nered in the literature, but it is related to the other con-\\ntrastive categories, and useful to us because we think\\nit can be exploited for search of differences and rival\\napproaches.\\n\\nThe next set of categories we propose concerns pos-\\nitive sentiment expressed towards a citation, or a state-\\nment that the other work is actively used in the cur-\\nrent work (which is the ultimate praise). Like Spiegel-\\nR\\xc2\\xa4using, we are interested in use of data and methods\\n(her categories 4, 5, 6, 7), but we cluster different us-\\nages together and instead differentiate unchanged use\\n(PUse) from use with adaptations (PModi). Work\\nwhich is stated as the explicit starting point or intellec-\\ntual ancestry is marked with our category PBas (her\\ncategory 2).\\nIf a claim in the literature is used to\\nstrengthen the authors\\xe2\\x80\\x99 argument, this is expressed in\\nher category 8, and vice versa, category 11. We col-\\nlapse these two in our category PSup. We use two\\ncategories she does not have de(cid:2)nitions for, namely\\nsimilarity of (aspect of) approach to other approach\\n(PSim), and motivation of approach used or problem\\naddressed (PMot). We found evidence for prototypi-\\ncal use of these citation functions in our texts. How-\\never, we found little evidence for her categories 12 or\\n13 (disproval or new interpretation of claims in cited\\nliterature), and we decided against a (cid:147)state-of-the-art(cid:148)\\ncategory (her category 1), which would have been in\\ncon(cid:3)ict with our PMot de(cid:2)nition in many cases.\\n\\nOur fourteenth category, Neut, bundles truly neutral\\ndescriptions of other researchers\\xe2\\x80\\x99 approaches with all\\nthose cases where the textual evidence for a citation\\nfunction was not enough to warrant annotation of that\\ncategory, and all other functions for which our scheme\\ndid not provide a speci(cid:2)c category. As stated above, we\\ndo in fact expect many of our citations to be neutral.\\n\\n\\x0c83\\n\\nCitation function is hard to annotate because it in\\nprinciple requires interpretation of author intentions\\n(what could the author\\xe2\\x80\\x99s intention have been in choos-\\ning a certain citation?). Typical results of earlier cita-\\ntion function studies are that the sociological aspect of\\nciting is not to be underestimated. One of our most fun-\\ndamental ideas for annotation is to only mark explicitly\\nsignalled citation functions. Our guidelines explicitly\\nstate that a general linguistic phrase such as (cid:147)better(cid:148)\\nor (cid:147)used by us(cid:148) must be present, in order to increase\\nobjectivity in (cid:2)nding citation function. Annotators are\\nencouraged to point to textual evidence they have for\\nassigning a particular function (and are asked to type\\nthe source of this evidence into the annotation tool for\\neach citation). Categories are de(cid:2)ned in terms of cer-\\ntain objective types of statements (e.g., there are 7 cases\\nfor PMot). Annotators can use general text interpreta-\\ntion principles when assigning the categories, but are\\nnot allowed to use in-depth knowledge of the (cid:2)eld or\\nof the authors.\\n\\nThere are other problematic aspects of the annota-\\ntion. Some concern the fact that authors do not al-\\nways state their purpose clearly. For instance, several\\nearlier studies found that negational citations are rare\\n(Moravcsik and Murugesan, 1975; Spiegel-R\\xc2\\xa4using,\\n1977); MacRoberts and MacRoberts (1984) argue that\\nthe reason for this is that they are potentially politically\\ndangerous, and that the authors go through lengths to\\ndiffuse the impact of negative references, hiding a neg-\\native point behind insincere praise, or diffusing the\\nthrust of criticism with perfunctory remarks.\\nIn our\\ndata we found ample evidence of this effect, illustrated\\nby the following example:\\n\\nHidden Markov Models (HMMs) (Huang\\net al. 1990) offer a powerful statistical ap-\\nproach to this problem, though it is unclear\\nhow they could be used to recognise the units\\nof interest to phonologists. (9410022, S-24)2\\n\\nIt is also sometimes extremely hard to distinguish\\nusage of a method from statements of similarity be-\\ntween a method and the own method. This happens\\nin cases where authors do not want to admit they are\\nusing somebody else\\xe2\\x80\\x99s method:\\n\\nThe same test was used in Abney and Light\\n(1999).\\n(0008020, S-151)\\nUni(cid:2)cation of indices proceeds in the same\\nmanner as uni(cid:2)cation of all other typed\\nfeature structures (Carpenter 1992).\\n\\n(0008023, S-87)\\n\\nIn this case, our annotators had to choose between\\n\\ncategories PSim and PUse.\\n\\nIt can also be hard to distinguish between continu-\\nation of somebody\\xe2\\x80\\x99s research (i.e., taking somebody\\xe2\\x80\\x99s\\n2In all corpus examples, numbers in brackets correspond\\nto the of(cid:2)cial Cmp lg archive number, (cid:147)S-(cid:148) numbers to sen-\\ntence numbers according to our preprocessing.\\n\\nresearch as starting point, as intellectual ancestry, i.e.\\nPBas) and simply using it (PUse). In principle, one\\nwould hope that annotation of all usage/positive cate-\\ngories (starting with P), if clustered together, should re-\\nsult in higher agreement (as they are similar, and as the\\nresulting scheme has fewer distinctions). We would ex-\\npect this to be the case in general, but as always, cases\\nexist where a con(cid:3)ict between a contrast (CoCo) and a\\nchange to a method (PModi) occur:\\n\\nIn contrast to McCarthy, Kay and Kiraz,\\nwe combine the three components into a sin-\\ngle projection.\\n(0006044, S-182)\\n\\nThe markable units in our scheme are a) all full cita-\\ntions (as recognized by our automatic citation proces-\\nsor on our corpus), and b) all names of authors of cited\\npapers anywhere in running text outside of a formal\\ncitation context (i.e., without date). Our citation pro-\\ncessor recognizes these latter names after parsing the\\ncitation list an marks them up. This is unusual in com-\\nparison to other citation indexers, but we believe these\\nnames function as important referents comparable in\\nimportance to formal citations. In principle, one could\\ngo even further as there are many other linguistic ex-\\npressions by which the authors could refer to other peo-\\nple\\xe2\\x80\\x99s work: pronouns, abbreviations such as (cid:147)Mueller\\nand Sag (1990), henceforth M & S(cid:148), and names of ap-\\nproaches or theories which are associated with partic-\\nular authors.\\nIf we could mark all of these up auto-\\nmatically (which is not technically possible), annota-\\ntion would become less dif(cid:2)cult to decide, but techni-\\ncal dif(cid:2)culty prevent us from recognizing these other\\ncases automatically. As a result, in these contexts it is\\nimpossible to annotate citation function directly on the\\nreferent, which sometimes causes problems. Because\\nthis means that annotators have to consider non-local\\ncontext, one markable may have different competing\\ncontexts with different potential citation functions, and\\nproblems about which context is (cid:147)stronger(cid:148) may oc-\\ncur. We have rules that context is to be constrained to\\nthe paragraph boundary, but for some categories paper-\\nwide information is required (e.g., for PMot, we need\\nto know that a praised approach is used by the authors,\\ninformation which may not be local in the paragraph).\\nAppendix A gives unambiguous example cases\\nwhere the citation function can be decided on the ba-\\nsis of the sentence alone, but Fig. 4 shows a more typ-\\nical example where more context is required to inter-\\npret the function. The evaluation of the citation Hin-\\ndle (1990) is contrastive; the evaluative statement is\\nfound 4 sentences after the sentence containing the ci-\\ntation3. It consists of a positive statement (agreement\\nwith authors\\xe2\\x80\\x99 view), followed by a weakness, under-\\nlined, which is the chosen category. This is marked on\\nthe nearest markable (Hindle, 3 sentences after the ci-\\ntation).\\n\\n3In Fig. 4, markables are shown in boxes, evaluative state-\\n\\nments underlined, and referents in bold face.\\n\\n\\x0c84\\n\\nS-5 Hindle (1990)/Neut proposed dealing with the\\nsparseness problem by estimating the likelihood of un-\\nseen events from that of (cid:147)similar(cid:148) events that have been\\nseen.\\nS-6 For instance, one may estimate the likelihood of a\\nparticular direct object for a verb from the likelihoods\\nof that direct object for similar verbs.\\nS-7 This requires a reasonable de(cid:2)nition of verb simi-\\nlarity and a similarity estimation method.\\nS-8 In Hindle/Weak \\xe2\\x80\\x99s proposal, words are similar\\nif we have strong statistical evidence that they tend to\\nparticipate in the same events.\\nS-9 His notion of similarity seems to agree with our in-\\ntuitions in many cases, but it is not clear how it can be\\nused directly to construct word classes and correspond-\\ning models of association.\\n(9408011)\\n\\nFigure 4: Annotation example: in(cid:3)uence of context\\n\\nA naive view on this annotation scheme could con-\\nsider the (cid:2)rst two sets of categories in our scheme as\\n(cid:147)negative(cid:148) and the third set of categories (cid:147)positive(cid:148).\\nThere is indeed a sentiment aspect to the interpretation\\nof citations, due to the fact that authors need to make\\na point in their paper and thus have a stance towards\\ntheir citations. But this is not the whole story: many\\nof our (cid:147)positive(cid:148) categories are more concerned with\\ndifferent ways in which the cited work is useful to the\\ncurrent work (which aspect of it is used, e.g., just a\\nde(cid:2)nition or the entire solution?), and many of the con-\\ntrastive statements have no negative connotation at all\\nand simply state a (value-free) difference between ap-\\nproaches. However, if one looks at the distribution of\\npositive and negative adjectives around citations, one\\nnotices a (non-trivial) connection between our task and\\nsentiment classi(cid:2)cation.\\n\\nThere are written guidelines of 25 pages, which in-\\nstruct the annotators to only assign one category per\\ncitation, and to skim-read the paper before annotation.\\nThe guidelines provide a decision tree and give deci-\\nsion aids in systematically ambiguous cases, but sub-\\njective judgement of the annotators is nevertheless nec-\\nessary to assign a single tag in an unseen context. We\\nimplemented an annotation tool based on XML/XSLT\\ntechnology, which allows us to use any web browser to\\ninteractively assign one of the 12 tags (presented as a\\npull-down list) to each citation.\\n\\n3 Data\\nThe data we used came from the CmpLg (Computation\\nand Language archive; 320 conference articles in com-\\nputational linguistics). The articles are in XML format.\\nHeadlines, titles, authors and reference list items are\\nautomatically marked up with the corresponding tags.\\nReference lists are parsed, and cited authors\\xe2\\x80\\x99 names\\nare identi(cid:2)ed. Our citation parser then applies regu-\\nlar patterns and (cid:2)nds citations and other occurrences of\\nthe names of cited authors (without a date) in running\\ntext and marks them up. Self-citations are detected by\\n\\noverlap of citing and cited authors. The citation pro-\\ncessor developped in our group (Ritchie et al., 2006)\\nachieves high accuracy for this task (96% of citations\\nrecognized, provided the reference list was error-free).\\nOn average, our papers contain 26.8 citation instances\\nin running text4.\\n4 Human Annotation: results\\nIn order to machine learn citation function, we are\\nin the process of creating a corpus of scienti(cid:2)c arti-\\ncles with human annotated citations, according to the\\nscheme discussed before. Here we report preliminary\\nresults with that scheme, with three annotators who are\\ndevelopers of the scheme.\\n\\nIn our experiment, the annotators independently an-\\nnotated 26 conference articles with this scheme, on the\\nbasis of guidelines which were frozen once annotation\\nstarted5. The data used for the experiment contained a\\ntotal of 120,000 running words and 548 citations.\\n\\nThe relative frequency of each category observed in\\nthe annotation is listed in Fig. 5. As expected, the dis-\\ntribution is very skewed, with more than 60% of the\\ncitations of category Neut.6 What is interesting is the\\nrelatively high frequency of usage categories (PUse,\\nPModi, PBas) with a total of 18.9%. There is\\na relatively low frequency of clearly negative cita-\\ntions (Weak, CoCoR-, total of 4.1%), whereas the\\nneutral(cid:150)contrastive categories (CoCoGM, CoCoR0,\\nCoCoXY) are slightly more frequent at 7.6%. This\\nis in concordance with earlier annotation experiments\\n(Moravcsik and Murugesan, 1975; Spiegel-R\\xc2\\xa4using,\\n1977).\\n\\nWe reached an inter-annotator agreement of K=.72\\n(n=12;N=548;k=3)7. This is comparable to aggreement\\non other discourse annotation tasks such as dialogue\\nact parsing and Argumentative Zoning (Teufel et al.,\\n1999). We consider the agreement quite good, consid-\\nering the number of categories and the dif(cid:2)culties (e.g.,\\nnon-local dependencies) of the task.\\n\\nThe annotators are obviously still disagreeing on\\nsome categories. We were wondering to what de-\\ngree the (cid:2)ne granularity of the scheme is a prob-\\nlem. When we collapsed the obvious similar cat-\\negories (all P categories into one category, and\\nall CoCo categories into another) to give four top\\nlevel categories (Weak, Positive, Contrast,\\nNeutral),\\nthis only raised kappa to 0.76. This\\n\\n4As opposed to reference list items, which are fewer.\\n5The development of the scheme was done with 40+ dif-\\n\\nferent articles.\\n\\n6Spiegel-R\\xc2\\xa4using found that out of 2309 citations she ex-\\n\\namined, 80% substantiated statements.\\n\\n7Following Carletta (1996), we measure agreement in\\nKappa, which follows the formula K =\\n1\\xe2\\x88\\x92P (E) where\\nP(A) is observed, and P(E) expected agreement. Kappa\\nranges between -1 and 1. K=0 means agreement is only as\\nexpected by chance. Generally, Kappas of 0.8 are considered\\nstable, and Kappas of .69 as marginally stable, according to\\nthe strictest scheme applied in the (cid:2)eld.\\n\\nP (A)\\xe2\\x88\\x92P (E)\\n\\n\\x0c85\\n\\nNeut\\nPUse\\n62.7% 15.8%\\n\\nCoCoGM PSim Weak\\n3.1%\\n\\n3.9%\\n\\n3.8%\\n\\nCoCoXY\\n\\n2.9%\\n\\nPMot\\n2.2%\\n\\nPModi\\n1.6%\\n\\nPBas\\nPSup\\n1.5% 1.1%\\n\\nCoCo-\\n1.0%\\n\\nCoCoR0\\n0.8%\\n\\nFigure 5: Distribution of the categories\\n\\nWeak CoCo- CoCoGM CoCoR0 CoCoXY PUse PBas PModi\\n\\nPMot PSim PSup Neut\\n3\\n\\nWeak\\nCoCo-\\nCoCoGM\\nCoCoR0\\nCoCoXY\\nPUse\\nPBas\\nPModi\\nPMot\\nPSim\\nPSup\\nNeut\\n\\n5\\n\\n6\\n\\n1\\n\\n1\\n\\n3\\n23\\n\\n10\\n\\n4\\n\\n6\\n\\n1\\n\\n4\\n\\n86\\n\\n3\\n2\\n17\\n\\n6\\n3\\n\\n1\\n\\n3\\n\\n13\\n\\n6\\n\\n3\\n\\n2\\n\\n20\\n\\n4\\n\\n1\\n\\n1\\n\\n12\\n2\\n\\n4\\n5\\n\\n287\\n\\nFigure 6: Confusion matrix between two annotators\\n\\npoints to the fact that most of our annotators disagreed\\nabout whether to assign a more informative category\\nor Neut, the neutral fall-back category. Unfortunately,\\nKappa is only partially sensitive to such specialised dis-\\nagreements. While it will reward agreement with in-\\nfrequent categories more than agreement with frequent\\ncategories, it nevertheless does not allow us to weight\\ndisagreements we care less about (Neut vs more in-\\nformative category) less than disagreements we do care\\na lot about (informative categories which are mutually\\nexclusive, such as Weak and PSim).\\n\\nFig. 6 shows a confusion matrix between the two an-\\nnotators who agreed most with each other. This again\\npoints to the fact that a large proportion of the confu-\\nsion involves an informative category and Neut. The\\nissue with Neut and Weak is a point at hand: au-\\nthors seem to often (deliberately or not) mask their in-\\ntended citation function with seemingly neutral state-\\nments. Many statements of weakness of other ap-\\nproaches were stated in such caged terms that our anno-\\ntators disagreed about whether the signals given were\\n(cid:147)explicit(cid:148) enough.\\n\\nWhile our focus is not sentiment analysis, it is pos-\\nsible to con(cid:3)ate our 12 categories into three: positive,\\nweakness and neutral by the following mapping:\\n\\nOld Categories New Category\\nWeak, CoCo- Negative\\nPositive\\nCoCoGM, CoCoR0, CoCoXY, Neut Neutral\\n\\nPMot, PUse, PBas, PModi, PSim, PSup\\n\\nWeakness\\nPositive\\nNeutral\\n\\nWeakness\\n\\n9\\n\\n4\\n\\nPositive Neutral\\n\\n1\\n140\\n30\\n\\n12\\n13\\n339\\n\\nFigure 7: Confusion matrix between two annotators;\\ncategories collapsed to re(cid:3)ect sentiment\\n\\nhave only one case of confusion between positive and\\nnegative references to cited work. The vast majority of\\ndisagreements re(cid:3)ects genuine ambiguity as to whether\\nthe authors were trying to stay neutral or express a sen-\\ntiment.\\n\\nDistinction\\nPMot v. all others\\nCoCoGM v. all others\\nPUse v. all others\\nCoCoR0 v. all others\\nNeut v. all others\\nPSim v. all others\\nPModi v. all others\\nCoCoXY v. all others\\nWeak v. all others\\nCoCo- v. all others\\nPBas v. all others\\nPSup v. all others\\n\\nKappa\\n.790\\n.765\\n.761\\n.746\\n.742\\n.649\\n.553\\n.553\\n.522\\n.462\\n.414\\n.268\\n\\nFigure 8: Distinctiveness of categories\\n\\nThus negative contrasts and weaknesses are grouped\\ninto Negative, while neutral contrasts are grouped\\ninto Neutral. All the positive classes are con(cid:3)ated\\ninto Positive. This resulted in kappa=0.75 for three\\nannotators.\\n\\nFig. 7 shows the confusion matrix between two an-\\nnotators for this sentiment classi(cid:2)cation. Fig. 7 is par-\\nticularly instructive, because it shows that annotators\\n\\nIn an attempt to determine how well each cate-\\ngory was de(cid:2)ned, we created arti(cid:2)cial splits of the\\ndata into binary distinctions: each category versus a\\nsuper-category consisting of all the other collapsed cat-\\negories. The kappas measured on these datasets are\\ngiven in Fig. 8. The higher they are, the better the anno-\\ntators could distinguish the given category from all the\\nother categories. We can see that out of the informa-\\n\\n\\x0c86\\n\\ntive categories, four are de(cid:2)ned at least as well as the\\noverall distinction (i.e. above the line in Fig. 8: PMot,\\nPUse, CoCoGM and CoCoR0. This is encouraging,\\nas the application of citation maps is almost entirely\\ncentered around usage and contrast. However, the se-\\nmantics of some categories are less well-understood by\\nour annotators: in particular PSup (where the dif(cid:2)culty\\nlies in what an annotator understands as (cid:147)mutual sup-\\nport(cid:148) of two theories), and (unfortunately) PBas. The\\nproblem with PBas is that its distinction from PUse is\\nbased on subjective judgement of whether the authors\\nuse a part of somebody\\xe2\\x80\\x99s previous work, or base them-\\nselves entirely on this previous work (i.e., see them-\\nselves as following in the same intellectual framework).\\nAnother problem concerns the low distinctivity for the\\nclearly negative categories CoCo- and Weak. This is\\nin line with MacRoberts and MacRoberts\\xe2\\x80\\x99 hypothesis\\nthat criticism is often hedged and not clearly lexically\\nsignalled, which makes it more dif(cid:2)cult to reliably an-\\nnotate such citations.\\n5 Conclusion\\nWe have described a new task: human annotation of\\ncitation function, a phenomenon which we believe to\\nbe closely related to the overall discourse structure of\\nscienti(cid:2)c articles. Our annotation scheme concentrates\\non contrast, weaknesses of other work, similarities be-\\ntween work and usage of other work. One of its prin-\\nciples is the fact that relations are only to be marked if\\nthey are explicitly signalled. Here, we report positive\\nresults in terms of interannotator agreement.\\n\\nFuture work on the annotation scheme will concen-\\ntrate on improving guidelines for currently suboptimal\\ncategories, and on measuring intra-annotator agree-\\nment and inter-annotator agreement with naive annota-\\ntors. We are also currently investigating how well our\\nscheme will work on text from a different discipline,\\nnamely chemistry. Work on applying machine learning\\ntechniques for automatic citation classi(cid:2)cation is cur-\\nrently underway (Teufel et al., 2006); the agreement\\nof one annotator and the system is currently K=.57,\\nleaving plenty of room for improvement in comparison\\nwith the human annotation results presented here.\\n6 Acknowledgements\\nThis work was funded by the EPSRC projects\\nCITRAZ (GR/S27832/01, (cid:147)Rhetorical Citation Maps\\nand Domain-independent Argumentative Zoning(cid:148)) and\\nSCIBORG (EP/C010035/1, (cid:147)Extracting the Science\\nfrom Scienti(cid:2)c Publications(cid:148)).\\n\\nReferences\\nSusan Bonzi. 1982. Characteristics of a literature as predic-\\ntors of relatedness between cited and citing works. JASIS,\\n33(4):208(cid:150)216.\\n\\nJean Carletta. 1996. Assessing agreement on classi(cid:2)cation\\n\\ntasks: The kappa statistic. Computational Linguistics,\\n22(2):249(cid:150)254.\\n\\nDaryl E. Chubin and S. D. Moitra. 1975. Content analysis\\nof references: Adjunct or alternative to citation counting?\\nSocial Studies of Science, 5(4):423(cid:150)441.\\n\\nCarolyn O. Frost. 1979. The use of citations in literary re-\\nsearch: A preliminary classi(cid:2)cation of citation functions.\\nLibrary Quarterly, 49:405.\\n\\nC. Lee Giles, Kurt D. Bollacker, and Steve Lawrence. 1998.\\nCiteseer: An automatic citation indexing system. In Pro-\\nceedings of the Third ACM Conference on Digital Li-\\nbraries, pages 89(cid:150)98.\\n\\nJorge E. Hirsch. 2005. An index to quantify an individ-\\nual\\xe2\\x80\\x99s scienti(cid:2)c research output. Proceedings of the Na-\\ntional Academy of Sciences of the United Stated of Amer-\\nica (PNAS), 102(46).\\n\\nTerttu Luukkonen. 1992. Is scientists\\xe2\\x80\\x99 publishing behaviour\\n\\nreward-seeking? Scientometrics, 24:297(cid:150)319.\\n\\nMichael H. MacRoberts and Barbara R. MacRoberts. 1984.\\nThe negational reference: Or the art of dissembling. So-\\ncial Studies of Science, 14:91(cid:150)94.\\n\\nMichael J. Moravcsik and Poovanalingan Murugesan. 1975.\\nSome results on the function and quality of citations. So-\\ncial Studies of Science, 5:88(cid:150)91.\\n\\nGreg Myers. 1992. In this paper we report...(cid:151)speech acts\\nand scienti(cid:2)c facts. Journal of Pragmatics, 17(4):295(cid:150)\\n313.\\n\\nJohn O\\xe2\\x80\\x99Connor. 1982. Citing statements: Computer recogni-\\ntion and use to improve retrieval. Information Processing\\nand Management, 18(3):125(cid:150)131.\\n\\nCharles Oppenheim and Susan P. Renn. 1978. Highly cited\\nold papers and the reasons why they continue to be cited.\\nJASIS, 29:226(cid:150)230.\\n\\nAnna Ritchie, Simone Teufel, and Steven Robertson. 2006.\\nCreating a test collection for citation-based IR experi-\\nments. In Proceedings of HLT-06.\\n\\nSimon Buckingham Shum. 1998. Evolving the web for sci-\\nenti(cid:2)c knowledge: First steps towards an (cid:147)HCI knowledge\\nweb(cid:148). Interfaces, British HCI Group Magazine, 39:16(cid:150)21.\\nIna Spiegel-R\\xc2\\xa4using. 1977. Bibliometric and content analy-\\n\\nsis. Social Studies of Science, 7:97(cid:150)113.\\n\\nJohn Swales. 1986. Citation analysis and discourse analysis.\\n\\nApplied Linguistics, 7(1):39(cid:150)56.\\n\\nJohn Swales, 1990. Genre Analysis: English in Academic\\nand Research Settings. Chapter 7: Research articles in En-\\nglish, pages 110(cid:150)176. Cambridge University Press, Cam-\\nbridge, UK.\\n\\nSimone Teufel, Jean Carletta, and Marc Moens. 1999. An\\nannotation scheme for discourse-level argumentation in re-\\nsearch articles. In Proceedings of the Ninth Meeting of the\\nEuropean Chapter of the Association for Computational\\nLinguistics (EACL-99), pages 110(cid:150)117.\\n\\nSimone Teufel, Advaith Siddharthan, and Dan Tidhar. 2006.\\nAutomatic classi(cid:2)cation of citation function. In Proceed-\\nings of EMNLP-06.\\n\\nSimone Teufel. 1999. Argumentative Zoning: Information\\nExtraction from Scienti(cid:2)c Text. Ph.D. thesis, School of\\nCognitive Science, University of Edinburgh, UK.\\n\\nMelvin Weinstock. 1971. Citation indexes. In Encyclopedia\\nof Library and Information Science, volume 5, pages 16(cid:150)\\n40. Dekker, New York, NY.\\n\\nHoward D. White. 2004. Citation analysis and discourse\\n\\nanalysis revisited. Applied Linguistics, 25(1):89(cid:150)116.\\n\\nJohn M. Ziman. 1968. Public Knowledge: An Essay Con-\\ncerning the Social Dimensions of Science. Cambridge\\nUniversity Press, Cambridge, UK.\\n\\n\\x0c87\\n\\nA Annotation examples\\n\\nWeak\\n\\nHowever, Koskenniemi himself understood that his initial implementation had signif-\\nicant limitations in handling non-concatenative morphotactic processes.\\n\\n(0006044, S-4)\\nCoCoGM The goals of the two papers are slightly different: Moore \\xe2\\x80\\x99s approach is designed to\\nreduce the total grammar size (i.e., the sum of the lengths of the productions), while\\nour approach minimizes the number of productions.\\n\\n(0008021, S-22)\\n\\nCoCoR0 This is similar to results in the literature (Ramshaw and Marcus 1995).\\n\\nPUse\\n\\nPBas\\n\\nCoCo-\\n\\n(0008022, S-147)\\nFor the Penn Treebank, Ratnaparkhi (1996) reports an accuracy of 96.6% using the\\nMaximum Entropy approach, our much simpler and therefore faster HMM approach\\ndelivers 96.7%.\\n(0003055, S-156)\\nCoCoXY Unlike previous approaches (Ellison 1994, Walther 1996), Karttunen \\xe2\\x80\\x99s approach\\nis encoded entirely in the (cid:2)nite state calculus, with no extra-logical procedures for\\ncounting constraint violations.\\n(0006038, S-5)\\nOur starting point is the work described in Ferro et al. (1999) , which used a fairly\\nsmall training set.\\n(0008004, S-11)\\nIn our application, we tried out the Learning Vector Quantization (LVQ) (Kohonen et\\nal. 1996).\\n(0003060, S-105)\\nIn our experiments, we have used a conjugate-gradient optimization program adapted\\nfrom the one presented in Press et al.\\n(0008028, S-72)\\nIt has also been shown that the combined accuracy of an ensemble of multiple clas-\\nsi(cid:2)ers is often signi(cid:2)cantly greater than that of any of the individual classi(cid:2)ers that\\nmake up the ensemble (e.g., Dietterich (1997)).\\n(0005006, S-9)\\nOur system is closely related to those proposed in Resnik (1997) and Abney and\\nLight (1999).\\n(0008020, S-24)\\nIn all experiments the SVM Light system outperformed other learning algorithms,\\nwhich con(cid:2)rms Yang and Liu \\xe2\\x80\\x99s (1999) results for SVMs fed with Reuters data.\\n\\nPModi\\n\\nPMot\\n\\nPSim\\n\\nPSup\\n\\n(0003060, S-141)\\nThe cosine metric and Jaccard\\xe2\\x80\\x99s coef(cid:2)cient are commonly used in information re-\\ntrieval as measures of association (Salton and McGill 1983).\\n(0001012, S-29)\\n\\nNeut\\n\\n\\x0c'\n"
     ]
    }
   ],
   "source": [
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import BytesIO\n",
    "\n",
    "def pdf_to_text(path):\n",
    "    manager = PDFResourceManager()\n",
    "    retstr = BytesIO()\n",
    "    layout = LAParams(all_texts=True)\n",
    "    device = TextConverter(manager, retstr, laparams=layout)\n",
    "    filepath = open(path, 'rb')\n",
    "    interpreter = PDFPageInterpreter(manager, device)\n",
    "\n",
    "    for page in PDFPage.get_pages(filepath, check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "\n",
    "    text = retstr.getvalue()\n",
    "\n",
    "    filepath.close()\n",
    "    device.close()\n",
    "    retstr.close()\n",
    "    return text\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    text = pdf_to_text(\"../src/data/tool/test.pdf\")\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "documents = {}\n",
    "#documents[\"d1\"] = open(\"../src/data/corpus/AH_1996_001.txt\",\"r\", encoding='utf-8', errors='ignore').read()\n",
    "documents[\"d2\"] = open(\"../src/data/corpus/AH_1996_002.txt\",\"r\", encoding='utf-8', errors='ignore').read()\n",
    "    \n",
    "REGEX_list = [\n",
    "    \"^References\"\n",
    "]\n",
    "d_filtered = {}\n",
    "for doc_k in documents:\n",
    "    doc_val = documents[doc_k]\n",
    "    doc_arr = doc_val.split(\"\\n\")\n",
    "    reg_row_index = 0\n",
    "    for row in doc_arr:\n",
    "        for d_reg_i in REGEX_list:\n",
    "            matches = re.match(d_reg_i,row)\n",
    "        if matches:\n",
    "            break\n",
    "        else:\n",
    "            reg_row_index = reg_row_index + 1\n",
    "    \n",
    "    fil_d = \"\"\n",
    "    if reg_row_index >= len(doc_arr) - 1:\n",
    "        fil_d = doc_val\n",
    "    else:\n",
    "        for row_index in range(0, reg_row_index):\n",
    "            if row_index == len(doc_arr) - 1:\n",
    "                fil_d = fil_d + doc_arr[row_index]\n",
    "            else:\n",
    "                fil_d = str(fil_d) + str(doc_arr[row_index]) + \"\\n\"\n",
    "    \n",
    "    d_filtered[doc_k] = fil_d\n",
    "\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    d_filtered[doc_k] = doc_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import webbrowser\n",
    "\n",
    "chrome_path = 'C:/Program Files (x86)/Google/Chrome/Application/chrome.exe %s' \n",
    "webbrowser.get(chrome_path).open(\"https://regex101.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python code to search .mp3 files in current \n",
    "# folder (We can change file type/name and path \n",
    "# according to the requirements. \n",
    "import os \n",
    "  \n",
    "# This is to get the directory that the program  \n",
    "# is currently running in. \n",
    "dir_path = os.path.dirname(os.path.abspath(os.sep)) \n",
    "  \n",
    "for root, dirs, files in os.walk(dir_path): \n",
    "    for file in files:  \n",
    "  \n",
    "        # change the extension from '.mp3' to  \n",
    "        # the one of your choice. \n",
    "        if file.startswith('chrome.exe'): \n",
    "            print(root+'/'+str(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accounting'}\n",
      "{'ma', \"you're\", 's', 'this', 'he', \"that'll\", 'not', \"hadn't\", 'yours', 't', 'o', \"it's\", 'did', 'had', 'same', 'after', \"needn't\", 'who', 'here', 'their', \"shan't\", \"hasn't\", \"she's\", 'up', 'needn', 'they', 'ain', 'to', 'during', \"mustn't\", 'these', 'itself', 'so', \"should've\", 'weren', 'it', 'her', 'me', 'each', 'other', \"mightn't\", 'theirs', 'before', 'both', \"you've\", 'himself', 'too', 'that', 'when', \"you'd\", 'by', 'couldn', 'no', 'is', 'at', 'further', 'you', 'on', 'than', 'can', 'why', 'over', \"haven't\", 'out', 'how', \"doesn't\", 'hers', 'y', 'yourselves', 'we', 'didn', 'him', 'with', 'as', 'from', \"don't\", \"wasn't\", 'll', 'above', 'mustn', 'very', 'have', 'until', \"didn't\", 've', 'm', 'wouldn', 'which', 'few', \"weren't\", 'down', 'own', 'been', 'but', 'those', 'between', \"aren't\", 'if', 'aren', 'once', 'yourself', 'herself', \"you'll\", 'having', 'wasn', 'am', 'a', 'its', 'again', 'be', 'below', 'whom', 'such', 'don', 'i', 'now', 'some', 'through', 'were', 'there', 'any', 'has', 're', 'them', 'shan', 'doesn', 'in', \"wouldn't\", 'into', 'most', 'will', 'does', 'being', \"isn't\", 'won', 'ours', 'should', \"shouldn't\", 'the', 'shouldn', 'or', 'our', 'because', 'isn', 'while', \"couldn't\", 'more', 'only', 'hasn', 'myself', 'd', 'was', 'she', 'against', 'mightn', 'under', 'and', 'where', 'themselves', 'my', 'are', 'hadn', \"won't\", 'your', 'for', 'ourselves', 'his', 'do', 'nor', 'what', 'an', 'off', 'just', 'about', 'doing', 'haven', 'then', 'all', 'of'}\n"
     ]
    }
   ],
   "source": [
    "p_stopwords = \"english\"\n",
    "stop = {\"accounting\"}\n",
    "if p_stopwords != \"none\":\n",
    "    pstopwords_set = set(stopwords.words(p_stopwords))\n",
    "    print(stop)\n",
    "    print(pstopwords_set)\n",
    "    stop = stop.union(pstopwords_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'accounting',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words(p_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dsadsa  dsad dsa .\n"
     ]
    }
   ],
   "source": [
    "a_regex = re.compile(r\"\\s\\w{1}\\s\")\n",
    "print(re.sub(a_regex,\" \", \"dsadsa  dsad d dsa .\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
